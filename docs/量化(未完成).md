[TOC]

# 量化

> Date: 2020/11/27

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201127104330.png)

X-CUBE-AI 支持两种量化方式：

1. [Keras 浮点模型](#1 Keras 浮点模型和对应相关的 json 文件)
2. [TF lite 模型](#2 量化训练后的模型)

# 1 Keras 浮点模型和对应相关的 json 文件

## 1.1 量化流程

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201127105305.png)

流程：(**目前卡点于 \<setup>.py 该文件**)

<center>model.h5 + config.json + setup.py 


<center>↓</center>

<center>new-model.h5 -重构模型文件 + model_Q.json -张量格式配置文件 + model_reference.npz + 'final_accuracy.txt'</center>

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201126114715.png 'config.json')

量化后生成的文件夹：

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201127110054.png)

```shell
# 量化过程
$ stm32ai quantize -q <conf_quant>.json

# 分析Keras后量化模型
$ stm32ai analyze -m <modified_model_file>.h5 -q <quant_file_desc>.json
...
input              : quantize_conv2d_1_input [784 items, 784 B, ai_i8, Q0.7, (28, 28, 1)]
input (total)      : 784 B
output             : softmax_8 [10 items, 40 B, ai_float, FLOAT32, (10,)]
output (total)     : 40 B
params #           : 1,199,882 items (4.58 MiB)
macc               : 12,088,202
weights (ro)       : 1,199,884 B (1171.76 KiB) (-75.00%)
activations (rw)   : 35,072 B (34.25 KiB)
ram (total)        : 35,896 B (35.05 KiB) = 35,072 + 784 + 40
...

$ stm32ai analyze -m <quantized_model_file>.tflite --allocate-inputs
...
input              : input_0 [2,107 items, 2.06 KiB, ai_u8, scale=0.5, zero=0, (49, 43, 1)]
input (total)      : 2.06 KiB
output             : nl_2 [4 items, 4 B, ai_u8, scale=0.00390625, zero=0, (4,)]
output (total)     : 4 B
params #           : 18,252 items (17.86 KiB)
macc               : 369,684
weights (ro)       : 18,288 B (17.86 KiB)
activations (rw)   : 6,860 B (6.70 KiB) *
ram (total)        : 6,864 B (6.70 KiB) = 6,860 + 0 + 4

 (*) inputs are placed in the activations buffer
...


# 使用自定义数据集验证量化模型
$ stm32ai validate -m <modified_model_file>.h5 -q <quant_file_desc>.json -vi test_data.npz
$ stm32ai validate -m <quantized_model>.tflite -vi test_data.npz

# 量化模型转c-model
$ stm32ai generate -m <model_file_path> -q <quant_file_desc>.json
```

## 1.2 量化方法

将 `float32` 转为 `int8`：**Qm,n**和**Integer**。

json 中的配置参数:

```json
{	
    "weights_integer_scheme": "SignedSymmetric",
	"activations_integer_scheme": "UnsignedAsymmetric"
}
```

SignedSymmetric: 有符号[-128, 127] 且对称（强制`zero_point` 为0）

UnsignedAsymmetric: 无符号[0, 255]且非对称

| scheme | weights                 | activations             |
| :----- | :---------------------- | :---------------------- |
| ua/ua  | unsigned and asymmetric | unsigned and asymmetric |
| ss/sa  | signed and symmetric    | signed and asymmetric   |
| ss/ua  | signed and symmetric    | unsigned and asymmetric |

- Qm,n 
  -  "per_channel": "False", 每层张量都一样
  -  仅考虑对称模式

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201126115114.png 'Qm,n')

<center>Qm,n</center>

- Integer
  - "per_channel": "False" or "True"

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201126115140.png 'Integer')

<center>Integer</center>






![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201127105041.png)



# 2 量化训练后的模型

> 参考资料：
>
> - [Post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization)
> - [Quantization aware training](https://www.tensorflow.org/model_optimization/guide/quantization/training)

先用**TensorFlow Lite Converter**将模型转换承`TF Lite`格式，然后在进行量化

三种训练后量化的方式：

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201130160301.png)

![](https://gitee.com/lebhoryi/PicGoPictureBed/raw/master/img/20201127172226.png)

## 2.1 Prepare tflite model

```python
import tensorflow as tf

tflite_model = tf.keras.models.load_model(path)

# 如果是tf1: tf.compat.v1.lite.TFLiteConverter.from_saved_model()
# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model(tflite_model) # path to the SavedModel directory
tflite_model = converter.convert()

# Convert the keras model.
converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

## 2.2 Quantization - Dynamic range quantization

只能用于CPU加速，

“动态范围”：根据激活函数的范围动态的将其转换为8bit整数

仅量化权重，从float32量化为int8，激活保持不变，模型减小了3/4 。

在推理的时候，把int8转回fp32，输入和输出都是浮点数

```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```

## 2.3 Quantization - Full integer quantization

```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```

- 输入输出是浮点型

```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# 定义示例数据生成器
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
    
# 为转换器提供示例数据
converter.representative_dataset = representative_dataset_gen
tflite_quant_model = converter.convert()
```

- 输入输出是整型

```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
def representative_dataset_gen():
  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    yield [input]
    
converter.representative_dataset = representative_dataset_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8  # or tf.uint8
converter.inference_output_type = tf.int8  # or tf.uint8
tflite_quant_model = converter.convert()
```

---

示例数据生成器函数样例：

```python
def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_images).batch(1).take(100):
    yield [input_value]
```

```python
def representative_dataset():
    for sample in samples:
        yield [sample.image]
```

## 2.4 Quantization - Float16 quantization

将权重与激活函数均转换为16位浮点数。

模型减小1/2。

```python
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]
tflite_quant_model = converter.convert()
```

## 2.5 STM32 + Tflite

X-CUBE-AI 所选择的量化方式为：`Full integer quantization`

示例代码如下：**难点在于示例数据生成器函数**

```python
def representative_dataset_gen():
  data = tload(...)

  for _ in range(num_calibration_steps):
    # Get sample input data as a numpy array in a method of your choosing.
    input = get_sample(data)
    yield [input]

converter = tf.lite.TFLiteConverter.from_keras_model_file(<keras_model_path>)
# trick from 2.x environment
# converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(<keras_model_path>)
converter.representative_dataset = representative_dataset_gen
# This enables quantization
converter.optimizations = [tf.lite.Optimize.DEFAULT]
# This ensures that if any ops can't be quantized, the converter throws an error
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
# For full integer quantization, though supported types defaults to int8 only
converter.target_spec.supported_types = [tf.int8]
# These set the input and output tensors to uint8 (added in r2.3)
converter.inference_input_type = tf.uint8  # or tf.int8
converter.inference_output_type = tf.uint8  # or tf.int8
quant_model = converter.convert()

# Save t quatized file
with open(<tflite_quant_model_path>, "wb") as f:
    f.write(quant_model)
...
```

## 2.6 stm 官方资料中摘抄

Consequently, it is recommended to use also the services from the tf.keras module to design the user modules (`test_set_generation.py` and `quantizer_user_algo.py` modules) avoiding possible incompatible situation

建议还使用tf.keras模块中的服务来设计用户模块（`test_set_generation.py`和`quantizer_user_algo.py`模块）